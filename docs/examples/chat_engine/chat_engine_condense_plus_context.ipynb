{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "616a781c",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/examples/chat_engine/chat_engine_context.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18e20fbc-056b-44ac-b1fc-2d34b8e99bcc",
   "metadata": {},
   "source": [
    "\n",
    "# Chat Engine - Condense Plus Context Mode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b99eea02-429c-40e4-99be-b82a89c8d070",
   "metadata": {},
   "source": [
    "This is a multi-step chat mode built on top of a retriever over your data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34d34fcc-e247-4d55-ab16-c3d633e2385a",
   "metadata": {},
   "source": [
    "For each chat interaction:\n",
    "* First condense a conversation and latest user message to a standalone question\n",
    "* Then build a context for the standalone question from a retriever,\n",
    "* Then pass the context along with prompt and user message to LLM to generate a response."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1c3cbc6-98a8-4e0e-98eb-3c7fa09ba79f",
   "metadata": {},
   "source": [
    "This approach is simple, and works for questions directly related to the knowledge base and general interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca364545",
   "metadata": {},
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46eb19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (0.9.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index) (2.0.23)\n",
      "Requirement already satisfied: aiostream<0.6.0,>=0.5.2 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from llama-index) (0.5.2)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from llama-index) (4.12.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from llama-index) (0.5.14)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from llama-index) (1.2.14)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from llama-index) (2023.10.0)\n",
      "Requirement already satisfied: httpx in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from llama-index) (0.25.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from llama-index) (1.5.8)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from llama-index) (3.8.1)\n",
      "Requirement already satisfied: numpy in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from llama-index) (1.24.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from llama-index) (1.1.1)\n",
      "Requirement already satisfied: pandas in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from llama-index) (2.0.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from llama-index) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from llama-index) (0.5.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from llama-index) (4.8.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from llama-index) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from llama-index) (1.26.18)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from beautifulsoup4<5.0.0,>=4.12.2->llama-index) (2.5)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->llama-index) (3.20.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from deprecated>=1.2.9.3->llama-index) (1.15.0)\n",
      "Requirement already satisfied: click in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index) (4.66.1)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from openai>=1.1.0->llama-index) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from openai>=1.1.0->llama-index) (1.8.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from openai>=1.1.0->llama-index) (1.10.12)\n",
      "Requirement already satisfied: certifi in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from httpx->llama-index) (2023.7.22)\n",
      "Requirement already satisfied: httpcore in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from httpx->llama-index) (1.0.1)\n",
      "Requirement already satisfied: idna in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from httpx->llama-index) (3.4)\n",
      "Requirement already satisfied: sniffio in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from httpx->llama-index) (1.3.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index) (3.0.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from tiktoken>=0.3.3->llama-index) (2.31.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from typing-inspect>=0.8.0->llama-index) (1.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from pandas->llama-index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from pandas->llama-index) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from pandas->llama-index) (2023.3)\n",
      "Requirement already satisfied: exceptiongroup in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from anyio<4,>=3.5.0->openai>=1.1.0->llama-index) (1.1.3)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->llama-index) (23.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->llama-index) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken>=0.3.3->llama-index) (3.3.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/kapilmalik/Library/Caches/pypoetry/virtualenvs/llama-index-9DABVzYh-py3.9/lib/python3.9/site-packages (from httpcore->httpx->llama-index) (0.14.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79db0610",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff623699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-16 12:12:20--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 75042 (73K) [text/plain]\n",
      "Saving to: 'data/paul_graham/paul_graham_essay.txt'\n",
      "\n",
      "data/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.05s   \n",
      "\n",
      "2023-11-16 12:12:29 (1.39 MB/s) - 'data/paul_graham/paul_graham_essay.txt' saved [75042/75042]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p 'data/paul_graham/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b314f279-bf7f-4e67-9f66-ebf783f08d38",
   "metadata": {},
   "source": [
    "## Get started in 5 lines of code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40d3d9e4",
   "metadata": {},
   "source": [
    "Load data and build index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3237c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"API_KEY_HERE\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ac125a-79df-452d-9f58-ac4f30997acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\")\n",
    ")\n",
    "data = SimpleDirectoryReader(input_dir=\"./data/paul_graham/\").load_data()\n",
    "index = VectorStoreIndex.from_documents(data, service_context=service_context)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e58d7ad9-d246-477e-acac-894ad5402f24",
   "metadata": {},
   "source": [
    "Configure chat engine\n",
    "\n",
    "Since the context retrieved can take up a large amount of the available LLM context, let's ensure we configure a smaller limit to the chat history!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164ef191-f86a-4ce1-aa9d-64d61f29dd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.memory import ChatMemoryBuffer\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=1500)\n",
    "\n",
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode=\"condense_plus_context\",\n",
    "    memory=memory,\n",
    "    context_prompt=(\n",
    "        \"You are a chatbot, able to have normal interactions, as well as talk\"\n",
    "        \" about an essay discussing Paul Grahams life.\"\n",
    "        \"Here are the relevant documents for the context:\\n\"\n",
    "        \"{context_str}\"\n",
    "        \"\\nInstruction: Based on the above documents, provide a detailed answer for the user question below.\"\n",
    "    ),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63a4259d-89b5-49f8-b158-9eba5353d6f5",
   "metadata": {},
   "source": [
    "Chat with your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825b5bb3-37ff-4886-be2c-264584ca9eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.chat(\"Who are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fa4310-4dc5-4787-a073-755d2e0b4887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am an AI chatbot designed to assist and engage in conversations with users. I can provide information, answer questions, and engage in discussions on various topics. Is there anything specific you would like to know or discuss?\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67021e64-8665-4338-9fb4-c0f1d6361092",
   "metadata": {},
   "source": [
    "Ask a follow up question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6181319-5d76-48c4-a5d4-23c6e9bc5ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_2 = chat_engine.chat(\"What did Paul Graham do growing up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95045f5b-7964-4872-bc91-809d9debf1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Growing up, Paul Graham had two main interests: writing and programming. He initially wrote short stories, although he admits that they were not very good. At the age of 13 or 14, he started programming on an IBM 1401 computer that his school district used for data processing. However, he found it challenging to figure out what to do with the computer since the only input option was punched cards, and he didn't have any data stored on them. Despite this, he continued exploring programming.\n",
      "\n",
      "When microcomputers became available, Graham's interest in programming grew. He convinced his father to buy a TRS-80 computer, and that's when he really started programming. He wrote simple games, a program to predict rocket heights, and even a word processor that his father used to write a book. This experience with microcomputers allowed him to have a computer right in front of him, responding to his keystrokes, which was a significant change from the punch card system.\n",
      "\n",
      "Overall, Graham's early experiences with programming and writing laid the foundation for his future endeavors and shaped his passion for technology and entrepreneurship.\n"
     ]
    }
   ],
   "source": [
    "print(response_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cc02dd-90b7-4d63-bdb2-e4c4666f87ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.chat(\"Can you tell me more?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f8efbb-fcb0-4c58-b92b-d2264a7e7103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! In the essay discussing Paul Graham's life, he talks about his early experiences with writing and programming. Before college, he wrote short stories, although he admits they were not very good. At the same time, he started programming on an IBM 1401 computer in his school's basement. However, he found it challenging to figure out what to do with the computer since the only input was data stored on punched cards.\n",
      "\n",
      "Graham's interest in programming grew when microcomputers became available. He was impressed by a friend who built his own microcomputer from a kit. Eventually, Graham convinced his father to buy a TRS-80 computer, which he used to write simple games, a rocket prediction program, and even a word processor that his father used to write a book.\n",
      "\n",
      "Despite his growing passion for programming, Graham initially planned to study philosophy in college. He believed that philosophy dealt with ultimate truths compared to other fields that focused on domain knowledge. However, he found philosophy courses to be boring and switched to studying artificial intelligence (AI) instead.\n",
      "\n",
      "Graham's interest in AI was influenced by a novel called \"The Moon is a Harsh Mistress\" by Heinlein, which featured an intelligent computer named Mike. Additionally, a PBS documentary showcasing Terry Winograd using the SHRDLU program further fueled his fascination with AI. He believed that intelligent computers like Mike and advanced AI systems were not far off in the future.\n",
      "\n",
      "Overall, these early experiences with writing and programming set the foundation for Graham's future endeavors and shaped his career path.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2c68de8-af58-4f7e-8759-19fc072873fd",
   "metadata": {},
   "source": [
    "Reset conversation state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13cf082-1a91-43c5-8bad-76fa45be96f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627de435-d195-4dad-b314-a68e731979a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.chat(\"Hello! What do you know?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ef9e31-3cdb-4129-92f7-e61be201ea36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! As an AI chatbot, I have access to a wide range of information. I can provide general knowledge, answer questions, engage in conversations, and assist with various topics. Is there something specific you would like to know or discuss?\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a65ad1a2",
   "metadata": {},
   "source": [
    "## Streaming Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad272dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import (\n",
    "    ServiceContext,\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    set_global_service_context,\n",
    ")\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    ")\n",
    "set_global_service_context(service_context)\n",
    "\n",
    "data = SimpleDirectoryReader(input_dir=\"./data/paul_graham/\").load_data()\n",
    "\n",
    "index = VectorStoreIndex.from_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22605caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode=\"condense_plus_context\",\n",
    "    context_prompt=(\n",
    "        \"You are a chatbot, able to have normal interactions, as well as talk\"\n",
    "        \" about an essay discussing Paul Grahams life.\"\n",
    "        \"Here are the relevant documents for the context:\\n\"\n",
    "        \"{context_str}\"\n",
    "        \"\\nInstruction: Based on the above documents, provide a detailed answer for the user question below.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250abd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Y Combinator (YC), Paul Graham made a significant decision to step back from his role in YC and pursue other endeavors. In 2012, his mother had a stroke caused by colon cancer, which led him to reevaluate his priorities. He realized that YC was consuming more of his attention and that he was ready to hand over the reins to someone else.\n",
      "\n",
      "Paul approached Jessica Livingston, his wife and co-founder of YC, to take over as president, but she declined. Eventually, they recruited Sam Altman, who initially wanted to start a startup focused on nuclear reactors. However, Paul persisted in convincing Sam to join YC, and in October 2013, Sam agreed to become the president of YC.\n",
      "\n",
      "During the transition period, Paul gradually handed over the responsibilities of running YC to Sam, allowing him to learn the job. This allowed Paul to focus on his mother, who was battling cancer. Ultimately, Paul retired from his active role in YC, along with co-founder Robert Morris, while Jessica Livingston and Trevor Blackwell continued as ordinary partners.\n",
      "\n",
      "After leaving YC, Paul Graham embarked on new ventures. One notable project was starting his own investment firm. Alongside Jessica, Robert, and Trevor, he founded an investment firm to implement the ideas they had been discussing. This move allowed them to fund startups and provide the support they wished they had received when they were starting out.\n",
      "\n",
      "Additionally, Paul continued to write essays, which had been one of his ongoing projects even during his time at YC. These essays covered a wide range of topics, including startups, technology, and life lessons. Paul's essays gained significant popularity and became a valuable resource for aspiring entrepreneurs and tech enthusiasts.\n",
      "\n",
      "Overall, after YC, Paul Graham focused on his investment firm, writing essays, and exploring new opportunities outside of YC."
     ]
    }
   ],
   "source": [
    "response = chat_engine.stream_chat(\"What did Paul Graham do after YC?\")\n",
    "for token in response.response_gen:\n",
    "    print(token, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
